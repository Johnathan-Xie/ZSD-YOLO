{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sonic-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "import clip\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from utils.general import xywhn2xyxy, xywh2xyxy, xyxy2xywh, xyxy2xywhn\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import test  # import test.py to get mAP after each epoch\n",
    "from nltk.corpus import wordnet\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.general import non_max_suppression\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader, LoadZSD\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    fitness, strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr\n",
    "from utils.google_utils import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_images, plot_labels, plot_results, plot_evolution\n",
    "from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first, is_parallel\n",
    "from utils.wandb_logging.wandb_utils import WandbLogger, check_wandb_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "three-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'ViT-B/32']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latest-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "opt = Namespace(single_cls=False, zsd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "primary-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyp.scratch.yaml') as f:\n",
    "    hyp = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surrounded-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, hyp, device='cuda'):\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    model = Model(ckpt['model'].yaml, ch=3, anchors=hyp.get('anchors'), hyp=hyp).to(device)\n",
    "    state_dict = ckpt['model'].float().state_dict()\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southeast-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_embeddings(images, boxes, clip_model, preprocess):\n",
    "    all_embeddings = []\n",
    "    for i in range(len(boxes)):\n",
    "        bboxes = deepcopy(boxes[i]).type(torch.IntTensor)\n",
    "        regions = []\n",
    "        include = []\n",
    "        for j in range(len(bboxes)):\n",
    "            x1, y1, x2, y2 = [int(k) for k in bboxes[j]]\n",
    "            regions.append(preprocess(images[i][:, y1:y2, x1:x2].clone().detach().float() / 255))\n",
    "        if(len(regions)):\n",
    "            regions = torch.stack(regions).cuda()\n",
    "            with torch.no_grad():\n",
    "                all_embeddings.append(clip_model.visual(regions))\n",
    "        else:\n",
    "            all_embeddings.append(torch.zeros((0, 512)).cuda())\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "therapeutic-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annot_torch(annot, data, out_path):\n",
    "    paths = [os.path.join(out_path, i.split('/')[-1].split('.')[0] + '.pt') for i in data[2]]\n",
    "    for i in range(len(paths)):\n",
    "        torch.save(annot[i].cpu(), paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floral-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zsd_data(path, hyp, opt, out_path, imgsz=640, batch_size=16, model_path=None, clip_name='ViT-B/32', \n",
    "                      score_thresh=0.1, iou_thresh=0.1, loader=None, min_w=0, min_h=0, delete=False, test=False, remove_tiny=True):\n",
    "    if os.path.exists(out_path) and delete:\n",
    "        shutil.rmtree(out_path)\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    clip_model, preprocess = clip.load(clip_name)\n",
    "    model = load_model(model_path, hyp).eval() if model_path else None\n",
    "    gs = max(int(model.stride.max()), 32)  if model else 32\n",
    "    loader, _ = (loader, None) if loader else create_dataloader(path, imgsz, batch_size, gs, opt, hyp=hyp, workers=4)\n",
    "    preprocess.transforms = [Resize(size=(224, 224)), lambda x: x.type(torch.cuda.HalfTensor), preprocess.transforms[-1]]\n",
    "    removed_boxes, total_boxes, self_label_boxes = 0, 0, 0\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for data in pbar:\n",
    "        c_batch_size = len(data[0])\n",
    "        count_per_batch = [0, ] * c_batch_size\n",
    "        for i in data[1]:\n",
    "            count_per_batch[int(i[0])] += 1\n",
    "        split_boxes = data[1].split(count_per_batch)\n",
    "        for i in range(len(split_boxes)):\n",
    "            split_boxes[i][:, 2:] = xywhn2xyxy(split_boxes[i][:, 2:])\n",
    "        split_boxes = [torch.cat([i[..., 2:], torch.ones((i.shape[:-1] + (1, ))) + 0.1, i[..., 1].unsqueeze(-1)], dim=1).cuda() for i in split_boxes]\n",
    "        if model:\n",
    "            imgs = data[0].to('cuda', non_blocking=True).float() / 255\n",
    "            with torch.no_grad():\n",
    "                output = model(imgs)\n",
    "            for idx, i in enumerate(output[0]):\n",
    "                i[:, :4] = xywh2xyxy(i[:, :4])\n",
    "                i[:, 5] = -1\n",
    "            all_boxes = [torch.cat([output[0][i][:, :6], split_boxes[i]]) for i in range(len(split_boxes))]\n",
    "            all_boxes = [i[i[:, 4] > score_thresh] for i in all_boxes]\n",
    "            all_boxes = [i[nms(i[:, :4], i[:, 4], iou_threshold=iou_thresh)] for i in all_boxes]\n",
    "            all_boxes = [i[i[:, 4] < 1] for i in all_boxes]\n",
    "            all_boxes = [torch.cat([i[..., :-1], torch.zeros(i[...].shape[:-1] + (1, )).cuda() - 1], dim=-1) for i in all_boxes]\n",
    "            for i in range(len(all_boxes)):\n",
    "                mask = torch.Tensor([(int(j[2]) - int(j[0])) > min_w and (int(j[3]) - int(j[1])) > min_h for j in all_boxes[i]])\n",
    "                all_boxes[i] = all_boxes[i][mask.type(torch.BoolTensor)]\n",
    "            split_boxes = [torch.cat([split_boxes[i], all_boxes[i]]) for i in range(len(split_boxes))]\n",
    "        for i in range(len(split_boxes)):\n",
    "            split_boxes[i][:, 0] = torch.clip(split_boxes[i][:, 0], min=data[3][i][1][1][0], max=data[3][i][0][1] * data[3][i][1][0][1] + data[3][i][1][1][0])\n",
    "            split_boxes[i][:, 1] = torch.clip(split_boxes[i][:, 1], min=data[3][i][1][1][1], max=data[3][i][0][0] * data[3][i][1][0][0] + data[3][i][1][1][1])\n",
    "            split_boxes[i][:, 2] = torch.clip(split_boxes[i][:, 2], min=data[3][i][1][1][0], max=data[3][i][0][1] * data[3][i][1][0][1] + data[3][i][1][1][0])\n",
    "            split_boxes[i][:, 3] = torch.clip(split_boxes[i][:, 3], min=data[3][i][1][1][1], max=data[3][i][0][0] * data[3][i][1][0][0] + data[3][i][1][1][1])\n",
    "            if remove_tiny:\n",
    "                mask = torch.Tensor([(((j[2] - j[0]) > 1) and ((j[3] - j[1]) > 1)) for j in split_boxes[i]])\n",
    "                previous_len = len(split_boxes[i])\n",
    "                split_boxes[i] = split_boxes[i][mask.type(torch.BoolTensor)]\n",
    "                removed_boxes += previous_len - len(split_boxes[i])\n",
    "        embeddings = [torch.zeros((i.shape[0], 512)) for i in split_boxes] if test else extract_image_embeddings(data[0], [i[:, :4] for i in split_boxes], clip_model, preprocess)\n",
    "        #print([i.shape for i in embeddings])\n",
    "        for i in range(len(split_boxes)):\n",
    "            split_boxes[i][:, :4] = xyxy2xywhn(split_boxes[i][:, :4], w=data[3][i][0][1] * data[3][i][1][0][1], h=data[3][i][0][0] * data[3][i][1][0][0], padw=data[3][i][1][1][0], padh=data[3][i][1][1][1])\n",
    "        annot = [torch.cat([split_boxes[i][:, 5].unsqueeze(-1), split_boxes[i][:, :4], embeddings[i]], dim=1).cpu() for i in range(len(split_boxes))]\n",
    "        save_annot_torch(annot, data, out_path)\n",
    "        total_boxes += sum(len(i) for i in split_boxes)\n",
    "        for i in annot:\n",
    "            self_label_boxes += sum(i[:, 0] == -1)\n",
    "        pbar.desc = f'Total removed boxes: {removed_boxes}. Total generated boxes: {total_boxes}. Self-label boxes: {self_label_boxes}. Generating Embeddings to {out_path}.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e093426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'utils.datasets.LoadImagesAndLabels'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning 'data/coco/labels_2017_ovd_pretrain_split/train2017.cache' images and labels... 107761 found, 0 missing, 0 empty, 3 corrupted: 100%|███████████████████████████████| 107761/107761 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "loader, _ = create_dataloader('data/coco/images2014_zsd_val_65_15/train2014',\n",
    "                              640, 16, 32, opt, hyp=hyp, workers=8, raw=False,\n",
    "                              annot_folder='labels2014_zsd_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f96abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 650/650 items from weights/pretrained_weights/yolov5l.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total removed boxes: 25. Total generated boxes: 814951. Self-label boxes: 158797. Generating Embeddings to data/coco/labels2017_ovd_self_l_48_17/train2017.1: 100%|█████| 6735/6735 [47:24<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_zsd_data('data/coco/images2014_zsd_val_65_15/train2014', hyp, opt, \n",
    "                                      'data/coco/labels2014_zsd_self_test_l_65_15/train2014', model_path='weights/pretrained_weights/yolov5l.pt',\n",
    "                                      loader=loader, min_w=25, min_h=25, iou_thresh=0.2, score_thresh=0.3, delete=False, test=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5_env",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
