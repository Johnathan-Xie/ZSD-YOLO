{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sonic-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "import torchvision\n",
    "import clip\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from utils.general import xywhn2xyxy, xywh2xyxy, xyxy2xywh, xyxy2xywhn\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import test  # import test.py to get mAP after each epoch\n",
    "from nltk.corpus import wordnet\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.general import non_max_suppression\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader, LoadZSD\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    fitness, strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr\n",
    "from utils.google_utils import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_images, plot_labels, plot_results, plot_evolution\n",
    "from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first, is_parallel\n",
    "from utils.wandb_logging.wandb_utils import WandbLogger, check_wandb_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "three-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'ViT-B/32']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "primary-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/coco/coco_zsd_2014_val_65_15.yaml') as f:\n",
    "    meta = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5a7d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_names = [meta['all_names'][i] for i in meta['unseen_names']]\n",
    "seen_names = [meta['all_names'][i] for i in meta['seen_names']]\n",
    "all_names = meta['all_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b098bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/lvis_v1_val.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89617/1850944623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/lvis_v1_val.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdefinitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/lvis_v1_val.json'"
     ]
    }
   ],
   "source": [
    "with open('data/lvis_v1_val.json') as f:\n",
    "    definitions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "98467304",
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = {i: wordnet.synsets(i)[0].definition() if len(wordnet.synsets(i)) else '' for i in all_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "69e02344",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_names[10] = 'hotdog'\n",
    "unseen_names[12] = 'computer mouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "defs_and_all_names = [i + ', ' + defs[i] + ',' if defs.get(i) else i for i in all_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5bb04178",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = ['a photo of {} in the scene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "possible-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "excellent-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_classifier(classnames, templates):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "lesser-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:03<00:00, 20.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#seen_text_embeddings = zeroshot_classifier(seen_names, templates)\n",
    "#unseen_text_embeddings = zeroshot_classifier(unseen_names, templates)\n",
    "all_text_embeddings = zeroshot_classifier(defs_and_all_names, templates)\n",
    "#all_text_embeddings = zeroshot_classifier(meta['all_names'], templates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2765e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_text_embeddings.T, 'embeddings/all_coco_text_embeddings_65_15_zsd.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5_env",
   "language": "python",
   "name": "yolov5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
